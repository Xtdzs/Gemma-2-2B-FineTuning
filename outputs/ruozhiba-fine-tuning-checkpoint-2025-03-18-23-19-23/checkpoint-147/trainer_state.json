{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9966101694915255,
  "eval_steps": 500,
  "global_step": 147,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006779661016949152,
      "grad_norm": 10.783990859985352,
      "learning_rate": 0.00019863945578231293,
      "loss": 4.1296,
      "step": 1
    },
    {
      "epoch": 0.013559322033898305,
      "grad_norm": 6.174272537231445,
      "learning_rate": 0.00019727891156462587,
      "loss": 3.2516,
      "step": 2
    },
    {
      "epoch": 0.020338983050847456,
      "grad_norm": 3.484346866607666,
      "learning_rate": 0.0001959183673469388,
      "loss": 2.723,
      "step": 3
    },
    {
      "epoch": 0.02711864406779661,
      "grad_norm": 13.683080673217773,
      "learning_rate": 0.0001945578231292517,
      "loss": 3.0294,
      "step": 4
    },
    {
      "epoch": 0.03389830508474576,
      "grad_norm": 4.502930164337158,
      "learning_rate": 0.00019319727891156462,
      "loss": 2.818,
      "step": 5
    },
    {
      "epoch": 0.04067796610169491,
      "grad_norm": 3.8515872955322266,
      "learning_rate": 0.00019183673469387756,
      "loss": 2.7172,
      "step": 6
    },
    {
      "epoch": 0.04745762711864407,
      "grad_norm": 3.619436740875244,
      "learning_rate": 0.00019047619047619048,
      "loss": 2.7201,
      "step": 7
    },
    {
      "epoch": 0.05423728813559322,
      "grad_norm": 3.7802507877349854,
      "learning_rate": 0.00018911564625850343,
      "loss": 2.7495,
      "step": 8
    },
    {
      "epoch": 0.061016949152542375,
      "grad_norm": 4.167268753051758,
      "learning_rate": 0.00018775510204081634,
      "loss": 2.6656,
      "step": 9
    },
    {
      "epoch": 0.06779661016949153,
      "grad_norm": 3.676137685775757,
      "learning_rate": 0.00018639455782312926,
      "loss": 2.5228,
      "step": 10
    },
    {
      "epoch": 0.07457627118644068,
      "grad_norm": 3.7000629901885986,
      "learning_rate": 0.0001850340136054422,
      "loss": 2.7578,
      "step": 11
    },
    {
      "epoch": 0.08135593220338982,
      "grad_norm": 3.769599676132202,
      "learning_rate": 0.00018367346938775512,
      "loss": 2.6514,
      "step": 12
    },
    {
      "epoch": 0.08813559322033898,
      "grad_norm": 3.4557037353515625,
      "learning_rate": 0.00018231292517006804,
      "loss": 2.4371,
      "step": 13
    },
    {
      "epoch": 0.09491525423728814,
      "grad_norm": 3.468505620956421,
      "learning_rate": 0.00018095238095238095,
      "loss": 2.6758,
      "step": 14
    },
    {
      "epoch": 0.1016949152542373,
      "grad_norm": 3.3305320739746094,
      "learning_rate": 0.0001795918367346939,
      "loss": 2.617,
      "step": 15
    },
    {
      "epoch": 0.10847457627118644,
      "grad_norm": 3.288184881210327,
      "learning_rate": 0.00017823129251700681,
      "loss": 2.7372,
      "step": 16
    },
    {
      "epoch": 0.1152542372881356,
      "grad_norm": 3.2529404163360596,
      "learning_rate": 0.00017687074829931973,
      "loss": 2.6096,
      "step": 17
    },
    {
      "epoch": 0.12203389830508475,
      "grad_norm": 3.357499599456787,
      "learning_rate": 0.00017551020408163265,
      "loss": 2.5769,
      "step": 18
    },
    {
      "epoch": 0.1288135593220339,
      "grad_norm": 3.029550313949585,
      "learning_rate": 0.0001741496598639456,
      "loss": 2.5504,
      "step": 19
    },
    {
      "epoch": 0.13559322033898305,
      "grad_norm": 3.378175973892212,
      "learning_rate": 0.0001727891156462585,
      "loss": 2.774,
      "step": 20
    },
    {
      "epoch": 0.1423728813559322,
      "grad_norm": 3.260857105255127,
      "learning_rate": 0.00017142857142857143,
      "loss": 2.5215,
      "step": 21
    },
    {
      "epoch": 0.14915254237288136,
      "grad_norm": 3.3592615127563477,
      "learning_rate": 0.00017006802721088434,
      "loss": 2.674,
      "step": 22
    },
    {
      "epoch": 0.15593220338983052,
      "grad_norm": 3.1124448776245117,
      "learning_rate": 0.00016870748299319729,
      "loss": 2.81,
      "step": 23
    },
    {
      "epoch": 0.16271186440677965,
      "grad_norm": 3.141319513320923,
      "learning_rate": 0.00016734693877551023,
      "loss": 2.7238,
      "step": 24
    },
    {
      "epoch": 0.1694915254237288,
      "grad_norm": 3.4706785678863525,
      "learning_rate": 0.00016598639455782315,
      "loss": 2.5472,
      "step": 25
    },
    {
      "epoch": 0.17627118644067796,
      "grad_norm": 3.4273712635040283,
      "learning_rate": 0.00016462585034013606,
      "loss": 2.4719,
      "step": 26
    },
    {
      "epoch": 0.18305084745762712,
      "grad_norm": 3.684609889984131,
      "learning_rate": 0.00016326530612244898,
      "loss": 2.8463,
      "step": 27
    },
    {
      "epoch": 0.18983050847457628,
      "grad_norm": NaN,
      "learning_rate": 0.00016326530612244898,
      "loss": 2.481,
      "step": 28
    },
    {
      "epoch": 0.19661016949152543,
      "grad_norm": 3.350736379623413,
      "learning_rate": 0.00016190476190476192,
      "loss": 2.5462,
      "step": 29
    },
    {
      "epoch": 0.2033898305084746,
      "grad_norm": 3.8237996101379395,
      "learning_rate": 0.00016054421768707484,
      "loss": 2.7465,
      "step": 30
    },
    {
      "epoch": 0.21016949152542372,
      "grad_norm": 3.42665433883667,
      "learning_rate": 0.00015918367346938776,
      "loss": 2.3784,
      "step": 31
    },
    {
      "epoch": 0.21694915254237288,
      "grad_norm": 3.357567548751831,
      "learning_rate": 0.00015782312925170067,
      "loss": 2.7182,
      "step": 32
    },
    {
      "epoch": 0.22372881355932203,
      "grad_norm": 3.0885860919952393,
      "learning_rate": 0.00015646258503401362,
      "loss": 2.3491,
      "step": 33
    },
    {
      "epoch": 0.2305084745762712,
      "grad_norm": 3.7739386558532715,
      "learning_rate": 0.00015510204081632654,
      "loss": 2.6066,
      "step": 34
    },
    {
      "epoch": 0.23728813559322035,
      "grad_norm": 3.240816593170166,
      "learning_rate": 0.00015374149659863945,
      "loss": 2.4737,
      "step": 35
    },
    {
      "epoch": 0.2440677966101695,
      "grad_norm": 3.2478139400482178,
      "learning_rate": 0.00015238095238095237,
      "loss": 2.5843,
      "step": 36
    },
    {
      "epoch": 0.25084745762711863,
      "grad_norm": 3.2430827617645264,
      "learning_rate": 0.0001510204081632653,
      "loss": 2.8598,
      "step": 37
    },
    {
      "epoch": 0.2576271186440678,
      "grad_norm": 2.8782455921173096,
      "learning_rate": 0.00014965986394557826,
      "loss": 2.5742,
      "step": 38
    },
    {
      "epoch": 0.26440677966101694,
      "grad_norm": 3.4998161792755127,
      "learning_rate": 0.00014829931972789117,
      "loss": 2.3197,
      "step": 39
    },
    {
      "epoch": 0.2711864406779661,
      "grad_norm": 3.0260169506073,
      "learning_rate": 0.0001469387755102041,
      "loss": 2.4731,
      "step": 40
    },
    {
      "epoch": 0.27796610169491526,
      "grad_norm": 3.449937343597412,
      "learning_rate": 0.000145578231292517,
      "loss": 2.6263,
      "step": 41
    },
    {
      "epoch": 0.2847457627118644,
      "grad_norm": 2.916109800338745,
      "learning_rate": 0.00014421768707482995,
      "loss": 2.2228,
      "step": 42
    },
    {
      "epoch": 0.29152542372881357,
      "grad_norm": 3.4928832054138184,
      "learning_rate": 0.00014285714285714287,
      "loss": 2.7395,
      "step": 43
    },
    {
      "epoch": 0.2983050847457627,
      "grad_norm": 3.125126838684082,
      "learning_rate": 0.00014149659863945578,
      "loss": 2.6437,
      "step": 44
    },
    {
      "epoch": 0.3050847457627119,
      "grad_norm": 2.972482442855835,
      "learning_rate": 0.0001401360544217687,
      "loss": 2.4075,
      "step": 45
    },
    {
      "epoch": 0.31186440677966104,
      "grad_norm": 3.650397300720215,
      "learning_rate": 0.00013877551020408165,
      "loss": 2.3714,
      "step": 46
    },
    {
      "epoch": 0.31864406779661014,
      "grad_norm": 3.393062114715576,
      "learning_rate": 0.00013741496598639456,
      "loss": 2.607,
      "step": 47
    },
    {
      "epoch": 0.3254237288135593,
      "grad_norm": 3.8286232948303223,
      "learning_rate": 0.00013605442176870748,
      "loss": 2.9136,
      "step": 48
    },
    {
      "epoch": 0.33220338983050846,
      "grad_norm": 4.257607936859131,
      "learning_rate": 0.0001346938775510204,
      "loss": 2.983,
      "step": 49
    },
    {
      "epoch": 0.3389830508474576,
      "grad_norm": 3.1607608795166016,
      "learning_rate": 0.00013333333333333334,
      "loss": 2.2807,
      "step": 50
    },
    {
      "epoch": 0.34576271186440677,
      "grad_norm": 2.862065315246582,
      "learning_rate": 0.00013197278911564626,
      "loss": 2.2456,
      "step": 51
    },
    {
      "epoch": 0.3525423728813559,
      "grad_norm": 3.7494211196899414,
      "learning_rate": 0.00013061224489795917,
      "loss": 2.6405,
      "step": 52
    },
    {
      "epoch": 0.3593220338983051,
      "grad_norm": 3.65095591545105,
      "learning_rate": 0.00012925170068027212,
      "loss": 2.7578,
      "step": 53
    },
    {
      "epoch": 0.36610169491525424,
      "grad_norm": 3.7978971004486084,
      "learning_rate": 0.00012789115646258506,
      "loss": 2.8008,
      "step": 54
    },
    {
      "epoch": 0.3728813559322034,
      "grad_norm": 3.765174150466919,
      "learning_rate": 0.00012653061224489798,
      "loss": 2.7611,
      "step": 55
    },
    {
      "epoch": 0.37966101694915255,
      "grad_norm": 3.261976480484009,
      "learning_rate": 0.0001251700680272109,
      "loss": 2.3591,
      "step": 56
    },
    {
      "epoch": 0.3864406779661017,
      "grad_norm": 3.339191436767578,
      "learning_rate": 0.0001238095238095238,
      "loss": 2.4369,
      "step": 57
    },
    {
      "epoch": 0.39322033898305087,
      "grad_norm": 3.130650043487549,
      "learning_rate": 0.00012244897959183676,
      "loss": 2.3856,
      "step": 58
    },
    {
      "epoch": 0.4,
      "grad_norm": 3.1619725227355957,
      "learning_rate": 0.00012108843537414967,
      "loss": 2.2597,
      "step": 59
    },
    {
      "epoch": 0.4067796610169492,
      "grad_norm": 3.705864906311035,
      "learning_rate": 0.00011972789115646259,
      "loss": 2.6341,
      "step": 60
    },
    {
      "epoch": 0.4135593220338983,
      "grad_norm": 3.444697856903076,
      "learning_rate": 0.00011836734693877552,
      "loss": 2.5747,
      "step": 61
    },
    {
      "epoch": 0.42033898305084744,
      "grad_norm": 3.2378275394439697,
      "learning_rate": 0.00011700680272108844,
      "loss": 2.4129,
      "step": 62
    },
    {
      "epoch": 0.4271186440677966,
      "grad_norm": 3.1431031227111816,
      "learning_rate": 0.00011564625850340137,
      "loss": 2.2679,
      "step": 63
    },
    {
      "epoch": 0.43389830508474575,
      "grad_norm": 3.577998399734497,
      "learning_rate": 0.00011428571428571428,
      "loss": 2.453,
      "step": 64
    },
    {
      "epoch": 0.4406779661016949,
      "grad_norm": 4.063973426818848,
      "learning_rate": 0.00011292517006802721,
      "loss": 2.5614,
      "step": 65
    },
    {
      "epoch": 0.44745762711864406,
      "grad_norm": 3.3747737407684326,
      "learning_rate": 0.00011156462585034013,
      "loss": 2.3028,
      "step": 66
    },
    {
      "epoch": 0.4542372881355932,
      "grad_norm": 3.627352476119995,
      "learning_rate": 0.00011020408163265306,
      "loss": 2.5066,
      "step": 67
    },
    {
      "epoch": 0.4610169491525424,
      "grad_norm": 3.0506162643432617,
      "learning_rate": 0.000108843537414966,
      "loss": 2.3665,
      "step": 68
    },
    {
      "epoch": 0.46779661016949153,
      "grad_norm": 3.4400599002838135,
      "learning_rate": 0.00010748299319727892,
      "loss": 2.3967,
      "step": 69
    },
    {
      "epoch": 0.4745762711864407,
      "grad_norm": 3.564342737197876,
      "learning_rate": 0.00010612244897959185,
      "loss": 2.1724,
      "step": 70
    },
    {
      "epoch": 0.48135593220338985,
      "grad_norm": 3.232121467590332,
      "learning_rate": 0.00010476190476190477,
      "loss": 2.4863,
      "step": 71
    },
    {
      "epoch": 0.488135593220339,
      "grad_norm": 3.1064722537994385,
      "learning_rate": 0.0001034013605442177,
      "loss": 2.6514,
      "step": 72
    },
    {
      "epoch": 0.49491525423728816,
      "grad_norm": 3.1987271308898926,
      "learning_rate": 0.00010204081632653062,
      "loss": 2.426,
      "step": 73
    },
    {
      "epoch": 0.5016949152542373,
      "grad_norm": 2.87248158454895,
      "learning_rate": 0.00010068027210884355,
      "loss": 2.0734,
      "step": 74
    },
    {
      "epoch": 0.5084745762711864,
      "grad_norm": 3.135864019393921,
      "learning_rate": 9.931972789115646e-05,
      "loss": 2.4759,
      "step": 75
    },
    {
      "epoch": 0.5152542372881356,
      "grad_norm": 3.2931196689605713,
      "learning_rate": 9.79591836734694e-05,
      "loss": 2.5635,
      "step": 76
    },
    {
      "epoch": 0.5220338983050847,
      "grad_norm": 3.7353317737579346,
      "learning_rate": 9.659863945578231e-05,
      "loss": 2.6881,
      "step": 77
    },
    {
      "epoch": 0.5288135593220339,
      "grad_norm": 3.9601049423217773,
      "learning_rate": 9.523809523809524e-05,
      "loss": 2.9722,
      "step": 78
    },
    {
      "epoch": 0.535593220338983,
      "grad_norm": 3.131288528442383,
      "learning_rate": 9.387755102040817e-05,
      "loss": 2.3686,
      "step": 79
    },
    {
      "epoch": 0.5423728813559322,
      "grad_norm": 3.3473927974700928,
      "learning_rate": 9.25170068027211e-05,
      "loss": 2.4946,
      "step": 80
    },
    {
      "epoch": 0.5491525423728814,
      "grad_norm": 2.8864221572875977,
      "learning_rate": 9.115646258503402e-05,
      "loss": 2.163,
      "step": 81
    },
    {
      "epoch": 0.5559322033898305,
      "grad_norm": 2.9626986980438232,
      "learning_rate": 8.979591836734695e-05,
      "loss": 2.329,
      "step": 82
    },
    {
      "epoch": 0.5627118644067797,
      "grad_norm": 3.3128621578216553,
      "learning_rate": 8.843537414965987e-05,
      "loss": 2.5556,
      "step": 83
    },
    {
      "epoch": 0.5694915254237288,
      "grad_norm": 3.272590398788452,
      "learning_rate": 8.70748299319728e-05,
      "loss": 2.3978,
      "step": 84
    },
    {
      "epoch": 0.576271186440678,
      "grad_norm": 3.5872642993927,
      "learning_rate": 8.571428571428571e-05,
      "loss": 2.706,
      "step": 85
    },
    {
      "epoch": 0.5830508474576271,
      "grad_norm": 3.7672431468963623,
      "learning_rate": 8.435374149659864e-05,
      "loss": 2.4599,
      "step": 86
    },
    {
      "epoch": 0.5898305084745763,
      "grad_norm": 3.5068888664245605,
      "learning_rate": 8.299319727891157e-05,
      "loss": 2.8558,
      "step": 87
    },
    {
      "epoch": 0.5966101694915255,
      "grad_norm": 2.9458227157592773,
      "learning_rate": 8.163265306122449e-05,
      "loss": 2.2578,
      "step": 88
    },
    {
      "epoch": 0.6033898305084746,
      "grad_norm": 3.2947051525115967,
      "learning_rate": 8.027210884353742e-05,
      "loss": 2.349,
      "step": 89
    },
    {
      "epoch": 0.6101694915254238,
      "grad_norm": 2.919753313064575,
      "learning_rate": 7.891156462585034e-05,
      "loss": 2.2429,
      "step": 90
    },
    {
      "epoch": 0.6169491525423729,
      "grad_norm": 4.329668045043945,
      "learning_rate": 7.755102040816327e-05,
      "loss": 2.7252,
      "step": 91
    },
    {
      "epoch": 0.6237288135593221,
      "grad_norm": 3.688297748565674,
      "learning_rate": 7.619047619047618e-05,
      "loss": 2.7736,
      "step": 92
    },
    {
      "epoch": 0.6305084745762712,
      "grad_norm": 3.353517770767212,
      "learning_rate": 7.482993197278913e-05,
      "loss": 2.42,
      "step": 93
    },
    {
      "epoch": 0.6372881355932203,
      "grad_norm": 3.5664291381835938,
      "learning_rate": 7.346938775510205e-05,
      "loss": 2.5458,
      "step": 94
    },
    {
      "epoch": 0.6440677966101694,
      "grad_norm": 3.439791440963745,
      "learning_rate": 7.210884353741498e-05,
      "loss": 2.9588,
      "step": 95
    },
    {
      "epoch": 0.6508474576271186,
      "grad_norm": 3.407935380935669,
      "learning_rate": 7.074829931972789e-05,
      "loss": 2.284,
      "step": 96
    },
    {
      "epoch": 0.6576271186440678,
      "grad_norm": 2.717135429382324,
      "learning_rate": 6.938775510204082e-05,
      "loss": 2.0724,
      "step": 97
    },
    {
      "epoch": 0.6644067796610169,
      "grad_norm": 3.349139451980591,
      "learning_rate": 6.802721088435374e-05,
      "loss": 2.514,
      "step": 98
    },
    {
      "epoch": 0.6711864406779661,
      "grad_norm": 3.0837533473968506,
      "learning_rate": 6.666666666666667e-05,
      "loss": 2.3253,
      "step": 99
    },
    {
      "epoch": 0.6779661016949152,
      "grad_norm": 2.7387280464172363,
      "learning_rate": 6.530612244897959e-05,
      "loss": 2.2355,
      "step": 100
    },
    {
      "epoch": 0.6847457627118644,
      "grad_norm": 2.722590446472168,
      "learning_rate": 6.394557823129253e-05,
      "loss": 2.3013,
      "step": 101
    },
    {
      "epoch": 0.6915254237288135,
      "grad_norm": 3.1641793251037598,
      "learning_rate": 6.258503401360545e-05,
      "loss": 2.5222,
      "step": 102
    },
    {
      "epoch": 0.6983050847457627,
      "grad_norm": 3.4707720279693604,
      "learning_rate": 6.122448979591838e-05,
      "loss": 2.5509,
      "step": 103
    },
    {
      "epoch": 0.7050847457627119,
      "grad_norm": 2.875941038131714,
      "learning_rate": 5.9863945578231295e-05,
      "loss": 2.5105,
      "step": 104
    },
    {
      "epoch": 0.711864406779661,
      "grad_norm": 3.256082057952881,
      "learning_rate": 5.850340136054422e-05,
      "loss": 2.4644,
      "step": 105
    },
    {
      "epoch": 0.7186440677966102,
      "grad_norm": 3.6337807178497314,
      "learning_rate": 5.714285714285714e-05,
      "loss": 2.6544,
      "step": 106
    },
    {
      "epoch": 0.7254237288135593,
      "grad_norm": 3.4726059436798096,
      "learning_rate": 5.5782312925170065e-05,
      "loss": 2.642,
      "step": 107
    },
    {
      "epoch": 0.7322033898305085,
      "grad_norm": 3.6738834381103516,
      "learning_rate": 5.4421768707483e-05,
      "loss": 2.3313,
      "step": 108
    },
    {
      "epoch": 0.7389830508474576,
      "grad_norm": 2.9337003231048584,
      "learning_rate": 5.3061224489795926e-05,
      "loss": 2.319,
      "step": 109
    },
    {
      "epoch": 0.7457627118644068,
      "grad_norm": 3.331702470779419,
      "learning_rate": 5.170068027210885e-05,
      "loss": 2.4819,
      "step": 110
    },
    {
      "epoch": 0.752542372881356,
      "grad_norm": 2.602545738220215,
      "learning_rate": 5.034013605442177e-05,
      "loss": 2.3399,
      "step": 111
    },
    {
      "epoch": 0.7593220338983051,
      "grad_norm": 3.747077226638794,
      "learning_rate": 4.89795918367347e-05,
      "loss": 2.6334,
      "step": 112
    },
    {
      "epoch": 0.7661016949152543,
      "grad_norm": 3.532879114151001,
      "learning_rate": 4.761904761904762e-05,
      "loss": 2.4812,
      "step": 113
    },
    {
      "epoch": 0.7728813559322034,
      "grad_norm": 3.5940098762512207,
      "learning_rate": 4.625850340136055e-05,
      "loss": 2.3787,
      "step": 114
    },
    {
      "epoch": 0.7796610169491526,
      "grad_norm": 3.2873284816741943,
      "learning_rate": 4.4897959183673474e-05,
      "loss": 2.4461,
      "step": 115
    },
    {
      "epoch": 0.7864406779661017,
      "grad_norm": 3.2188832759857178,
      "learning_rate": 4.35374149659864e-05,
      "loss": 2.2361,
      "step": 116
    },
    {
      "epoch": 0.7932203389830509,
      "grad_norm": 2.93778920173645,
      "learning_rate": 4.217687074829932e-05,
      "loss": 2.1882,
      "step": 117
    },
    {
      "epoch": 0.8,
      "grad_norm": 3.597494602203369,
      "learning_rate": 4.0816326530612245e-05,
      "loss": 2.6149,
      "step": 118
    },
    {
      "epoch": 0.8067796610169492,
      "grad_norm": 2.863980293273926,
      "learning_rate": 3.945578231292517e-05,
      "loss": 2.1722,
      "step": 119
    },
    {
      "epoch": 0.8135593220338984,
      "grad_norm": 3.5051000118255615,
      "learning_rate": 3.809523809523809e-05,
      "loss": 2.4508,
      "step": 120
    },
    {
      "epoch": 0.8203389830508474,
      "grad_norm": 3.442276954650879,
      "learning_rate": 3.673469387755102e-05,
      "loss": 2.3758,
      "step": 121
    },
    {
      "epoch": 0.8271186440677966,
      "grad_norm": 3.0239005088806152,
      "learning_rate": 3.5374149659863946e-05,
      "loss": 2.1273,
      "step": 122
    },
    {
      "epoch": 0.8338983050847457,
      "grad_norm": 4.131231307983398,
      "learning_rate": 3.401360544217687e-05,
      "loss": 2.792,
      "step": 123
    },
    {
      "epoch": 0.8406779661016949,
      "grad_norm": 3.318063974380493,
      "learning_rate": 3.265306122448979e-05,
      "loss": 2.4933,
      "step": 124
    },
    {
      "epoch": 0.847457627118644,
      "grad_norm": 3.0810635089874268,
      "learning_rate": 3.1292517006802724e-05,
      "loss": 2.3187,
      "step": 125
    },
    {
      "epoch": 0.8542372881355932,
      "grad_norm": 3.7554633617401123,
      "learning_rate": 2.9931972789115647e-05,
      "loss": 2.539,
      "step": 126
    },
    {
      "epoch": 0.8610169491525423,
      "grad_norm": 3.937009811401367,
      "learning_rate": 2.857142857142857e-05,
      "loss": 2.5851,
      "step": 127
    },
    {
      "epoch": 0.8677966101694915,
      "grad_norm": 3.345367908477783,
      "learning_rate": 2.72108843537415e-05,
      "loss": 2.3957,
      "step": 128
    },
    {
      "epoch": 0.8745762711864407,
      "grad_norm": 3.166232109069824,
      "learning_rate": 2.5850340136054425e-05,
      "loss": 2.3712,
      "step": 129
    },
    {
      "epoch": 0.8813559322033898,
      "grad_norm": 3.735670804977417,
      "learning_rate": 2.448979591836735e-05,
      "loss": 2.3985,
      "step": 130
    },
    {
      "epoch": 0.888135593220339,
      "grad_norm": 3.7830793857574463,
      "learning_rate": 2.3129251700680275e-05,
      "loss": 2.7495,
      "step": 131
    },
    {
      "epoch": 0.8949152542372881,
      "grad_norm": 3.717452049255371,
      "learning_rate": 2.17687074829932e-05,
      "loss": 2.7134,
      "step": 132
    },
    {
      "epoch": 0.9016949152542373,
      "grad_norm": 3.558959484100342,
      "learning_rate": 2.0408163265306123e-05,
      "loss": 2.5076,
      "step": 133
    },
    {
      "epoch": 0.9084745762711864,
      "grad_norm": 3.178182363510132,
      "learning_rate": 1.9047619047619046e-05,
      "loss": 2.4747,
      "step": 134
    },
    {
      "epoch": 0.9152542372881356,
      "grad_norm": 3.017961263656616,
      "learning_rate": 1.7687074829931973e-05,
      "loss": 2.1859,
      "step": 135
    },
    {
      "epoch": 0.9220338983050848,
      "grad_norm": 3.2036352157592773,
      "learning_rate": 1.6326530612244897e-05,
      "loss": 2.3655,
      "step": 136
    },
    {
      "epoch": 0.9288135593220339,
      "grad_norm": 3.270772933959961,
      "learning_rate": 1.4965986394557824e-05,
      "loss": 2.4543,
      "step": 137
    },
    {
      "epoch": 0.9355932203389831,
      "grad_norm": 2.7827351093292236,
      "learning_rate": 1.360544217687075e-05,
      "loss": 2.4532,
      "step": 138
    },
    {
      "epoch": 0.9423728813559322,
      "grad_norm": 2.909757375717163,
      "learning_rate": 1.2244897959183674e-05,
      "loss": 2.4306,
      "step": 139
    },
    {
      "epoch": 0.9491525423728814,
      "grad_norm": 3.9816231727600098,
      "learning_rate": 1.08843537414966e-05,
      "loss": 2.6749,
      "step": 140
    },
    {
      "epoch": 0.9559322033898305,
      "grad_norm": 3.231692314147949,
      "learning_rate": 9.523809523809523e-06,
      "loss": 2.3637,
      "step": 141
    },
    {
      "epoch": 0.9627118644067797,
      "grad_norm": 4.149548053741455,
      "learning_rate": 8.163265306122448e-06,
      "loss": 2.7502,
      "step": 142
    },
    {
      "epoch": 0.9694915254237289,
      "grad_norm": 3.727574586868286,
      "learning_rate": 6.802721088435375e-06,
      "loss": 2.6684,
      "step": 143
    },
    {
      "epoch": 0.976271186440678,
      "grad_norm": 3.369499921798706,
      "learning_rate": 5.4421768707483e-06,
      "loss": 2.4454,
      "step": 144
    },
    {
      "epoch": 0.9830508474576272,
      "grad_norm": 3.2528536319732666,
      "learning_rate": 4.081632653061224e-06,
      "loss": 2.4756,
      "step": 145
    },
    {
      "epoch": 0.9898305084745763,
      "grad_norm": 2.9783308506011963,
      "learning_rate": 2.72108843537415e-06,
      "loss": 2.3278,
      "step": 146
    },
    {
      "epoch": 0.9966101694915255,
      "grad_norm": 3.258610963821411,
      "learning_rate": 1.360544217687075e-06,
      "loss": 2.4974,
      "step": 147
    }
  ],
  "logging_steps": 1,
  "max_steps": 147,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3897340545429504.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
